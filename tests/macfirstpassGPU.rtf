{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red224\green224\blue224;\red25\green29\blue39;}
{\*\expandedcolortbl;;\csgenericrgb\c87761\c87761\c87761;\csgenericrgb\c9839\c11388\c15260\c95000;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 \CocoaLigature0 corbinshanks@Corbins-MacBook-Pro llama.cpp % cd ~/llama.cpp\
./build/bin/llama-bench \\\
  -m /Users/corbinshanks/Documents/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\
  -p 64 \\\
  -n 128 \\\
  -b 256 \\\
  -ub 128 \\\
  -ngl 999 \\\
  -r 3\
\
cd: no such file or directory: /Users/corbinshanks/llama.cpp\
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\
ggml_metal_library_init: using embedded metal library\
ggml_metal_library_init: loaded in 7.948 sec\
ggml_metal_device_init: GPU name:   Apple M1 Pro\
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)\
ggml_metal_device_init: simdgroup reduction   = true\
ggml_metal_device_init: simdgroup matrix mul. = true\
ggml_metal_device_init: has unified memory    = true\
ggml_metal_device_init: has bfloat            = true\
ggml_metal_device_init: has tensor            = false\
ggml_metal_device_init: use residency sets    = true\
ggml_metal_device_init: use shared buffers    = true\
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 12713.12 MB\
| model                          |       size |     params | backend    | threads | n_batch | n_ubatch |            test |                  t/s |\
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------: | -------: | --------------: | -------------------: |\
| llama 1B Q4_K - Medium         | 636.18 MiB |     1.10 B | Metal,BLAS |       8 |     256 |      128 |            pp64 |       1432.99 \'b1 1.54 |\
| llama 1B Q4_K - Medium         | 636.18 MiB |     1.10 B | Metal,BLAS |       8 |     256 |      128 |           tg128 |        133.13 \'b1 0.20 |\
\
build: 877566d51 (7151)\
corbinshanks@Corbins-MacBook-Pro llama.cpp % cd ~/llama.cpp\
./build/bin/llama-bench \\\
  -m /Users/corbinshanks/Documents/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\
  -p 64 \\\
  -n 128 \\\
  -b 256 \\\
  -ub 128 \\\
  -ngl 999 \\\
  -r 3\
\
cd: no such file or directory: /Users/corbinshanks/llama.cpp\
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\
ggml_metal_library_init: using embedded metal library\
ggml_metal_library_init: loaded in 0.009 sec\
ggml_metal_device_init: GPU name:   Apple M1 Pro\
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)\
ggml_metal_device_init: simdgroup reduction   = true\
ggml_metal_device_init: simdgroup matrix mul. = true\
ggml_metal_device_init: has unified memory    = true\
ggml_metal_device_init: has bfloat            = true\
ggml_metal_device_init: has tensor            = false\
ggml_metal_device_init: use residency sets    = true\
ggml_metal_device_init: use shared buffers    = true\
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 12713.12 MB\
| model                          |       size |     params | backend    | threads | n_batch | n_ubatch |            test |                  t/s |\
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------: | -------: | --------------: | -------------------: |\
| llama 1B Q4_K - Medium         | 636.18 MiB |     1.10 B | Metal,BLAS |       8 |     256 |      128 |            pp64 |       1429.73 \'b1 2.40 |\
| llama 1B Q4_K - Medium         | 636.18 MiB |     1.10 B | Metal,BLAS |       8 |     256 |      128 |           tg128 |        133.38 \'b1 0.30 |\
\
build: 877566d51 (7151)\
corbinshanks@Corbins-MacBook-Pro llama.cpp % cd ~/llama.cpp\
./build/bin/llama-bench \\\
  -m /Users/corbinshanks/Documents/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\
  -p 64 \\\
  -n 128 \\\
  -b 256 \\\
  -ub 128 \\\
  -ngl 999 \\\
  -r 3\
\
cd: no such file or directory: /Users/corbinshanks/llama.cpp\
ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\
ggml_metal_library_init: using embedded metal library\
ggml_metal_library_init: loaded in 0.009 sec\
ggml_metal_device_init: GPU name:   Apple M1 Pro\
ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)\
ggml_metal_device_init: simdgroup reduction   = true\
ggml_metal_device_init: simdgroup matrix mul. = true\
ggml_metal_device_init: has unified memory    = true\
ggml_metal_device_init: has bfloat            = true\
ggml_metal_device_init: has tensor            = false\
ggml_metal_device_init: use residency sets    = true\
ggml_metal_device_init: use shared buffers    = true\
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 12713.12 MB\
| model                          |       size |     params | backend    | threads | n_batch | n_ubatch |            test |                  t/s |\
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------: | -------: | --------------: | -------------------: |\
| llama 1B Q4_K - Medium         | 636.18 MiB |     1.10 B | Metal,BLAS |       8 |     256 |      128 |            pp64 |       1430.81 \'b1 1.46 |\
| llama 1B Q4_K - Medium         | 636.18 MiB |     1.10 B | Metal,BLAS |       8 |     256 |      128 |           tg128 |        133.35 \'b1 0.30 |\
\
build: 877566d51 (7151)\
corbinshanks@Corbins-MacBook-Pro llama.cpp % }