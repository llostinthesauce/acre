export LD_LIBRARY_PATH=/usr/lib/aarch64-linux-gnu/tegra:${LD_LIBRARY_PATH}


model: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf | threads: 6 | n_gpu_layers: 12
 pass    ctx   max    load_s   infer_s   tokens   eval_tps   prompt_tps    rssΔ_MB      vram_MB    power_W   cpu_peak
---------------------------------------------------------------------------------------------------------------------
  1/3    512    64     0.677     1.311       28      21.36          n/a     316.98         n/a*        n/a      331.4
  2/3   1024   128     0.219     1.112       28      25.19          n/a      12.28         n/a*        n/a      323.4
  3/3   2048   256     0.226     1.148       28      24.38          n/a      11.53         n/a*        n/a      337.5


=== Performance Test (TinyLlama tiered) ===
model: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf | threads: 6 | n_gpu_layers: 20
 pass    ctx   max    load_s   infer_s   tokens   eval_tps   prompt_tps    rssΔ_MB      vram_MB    power_W   cpu_peak
---------------------------------------------------------------------------------------------------------------------
  1/3    512    64     0.288     2.598       64      24.64          n/a       5.38         n/a*        n/a      183.7
  2/3   1024   128     0.260     4.734      128      27.04          n/a       0.20         n/a*        n/a      179.7
  3/3   2048   256     0.276     7.306      256      35.04          n/a      11.83         n/a*        n/a      201.7


=== Performance Test (TinyLlama tiered) ===
model: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf | threads: 6 | n_gpu_layers: 50
 pass    ctx   max    load_s   infer_s   tokens   eval_tps   prompt_tps    rssΔ_MB      vram_MB    power_W   cpu_peak
---------------------------------------------------------------------------------------------------------------------
  1/3    512    64     0.296     1.809       64      35.38          n/a       4.03         n/a*        n/a       63.9
  2/3   1024   128     0.291     3.100      128      41.28          n/a       0.02         n/a*        n/a       61.9
  3/3   2048   256     0.307     5.682      256      45.06          n/a       1.07         n/a*        n/a       65.9

=== Performance Test (TinyLlama tiered) ===
model: models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf | threads: 6 | n_gpu_layers: 999
 pass    ctx   max    load_s   infer_s   tokens   eval_tps   prompt_tps    rssΔ_MB      vram_MB    power_W   cpu_peak
---------------------------------------------------------------------------------------------------------------------
  1/3    512    64     0.302     1.658       64      38.59          n/a       0.10         n/a*        n/a      936.2
  2/3   1024   128     0.314     3.077      128      41.60          n/a       0.02         n/a*        n/a       65.9
  3/3   2048   256     0.332     5.661      256      45.22          n/a       0.04         n/a*        n/a       67.9

